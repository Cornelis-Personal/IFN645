{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment 1\n",
    "Daniel Brandenburg n8867887"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Import plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Import ML libraries\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Set a random state value\n",
    "rs = 10\n",
    "\n",
    "#import libraries to visualize decision trees\n",
    "import pydot\n",
    "from io import StringIO\n",
    "\n",
    "\n",
    "# Ignore Warnings and ignore them\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreProcessing (data):\n",
    "    print(\"Pre-Processing Step\")\n",
    "    \n",
    "    # Check if there are any missing target variables\n",
    "    if data['IsBadBuy'].isna().any():\n",
    "        print(\"Missing Target Variables\")\n",
    "    else:\n",
    "        print(\"No missing Target Variables\")\n",
    "    \n",
    "    \n",
    "    # Handle Bad Columns drop Columns\n",
    "    data.drop(['PRIMEUNIT', 'AUCGUART', 'WheelTypeID', 'ForSale', 'PurchaseDate'], \n",
    "              axis=1, \n",
    "              inplace=True)\n",
    "    \n",
    "\n",
    "    # Handle Missing Values\n",
    "    i = 0            # Python's indexing starts at zero\n",
    "    for item in data['TopThreeAmericanName']:   # Python's for loops are a \"for each\" loop \n",
    "        if data['TopThreeAmericanName'][i] == np.nan and  data['Make'][i] == 'Hyundai':\n",
    "            data['TopThreeAmericanName'][i] = 'HYUNDAI'\n",
    "        i += 1\n",
    "        \n",
    "    i = 0            # Python's indexing starts at zero\n",
    "    for item in data['TopThreeAmericanName']:   # Python's for loops are a \"for each\" loop \n",
    "        if data['TopThreeAmericanName'][i] == np.nan and  data['Make'][i] == 'Jeep':\n",
    "            data['TopThreeAmericanName'][i] = 'JEEP'\n",
    "        i += 1\n",
    "        \n",
    "        \n",
    "    \"\"\" DOES THIS HAVE TO BE THERE \"\"\"\n",
    "    # print(data.groupby(['TopThreeAmericanName'])['Make'].value_counts())\n",
    "    \"\"\" DOES THIS HAVE TO BE THERE \"\"\"\n",
    "    \n",
    "    \n",
    "\n",
    "    # Standardise the capitilization across all object rows\n",
    "    obj_cols = data.select_dtypes(include='object').columns # Create a list of col names\n",
    "    for i in obj_cols:              # Interate over the obj_cols list\n",
    "        data[i] = data[i].str.upper()   # Convert all strings to uppercase  \n",
    "    # Standardize USA to AMERICA\n",
    "    data['Nationality'].replace({'USA' : 'AMERICAN'}, inplace = True)\n",
    "    \n",
    "    # Turn Transmission into a binary variable with Auto = 1 and Manual = 0\n",
    "    data.rename({'Transmission' : 'Auto'}, axis = 1, inplace = True)\n",
    "    # Replace binary columns with 1s and 0s\n",
    "    data['Auto'].replace({'MANUAL' : 0, 'AUTO' : 1}, inplace=True)\n",
    "    \n",
    "    # Remove NOT AVAIL in color and place it in the NaN section\n",
    "    data['Color'].replace({'NOT AVAIL': np.nan}, inplace = True)\n",
    "    \n",
    "    \n",
    "    \"\"\" I'll have to check with teach if this is correct\n",
    "    #This is the significance test for VNST\n",
    "    # Check to see if VNST is a statisically significant variable\n",
    "    # Create a distribution of IsBuyBad for VNST\n",
    "    VNST_badBuy = pd.crosstab(data['IsBadBuy'], data['VNST']).loc[0]\n",
    "    VNST_goodBuy = pd.crosstab(data['IsBadBuy'], data['VNST']).loc[1]\n",
    "    categoricalPlot('VNST')\n",
    "    \n",
    "    # Use a Chi2 test to test if there is any corrilation between them, if there is\n",
    "    # (p < 0.05) discarde the variable\n",
    "    fScore, pValue = stats.f_oneway(VNST_badBuy, VNST_goodBuy)\n",
    "    print(\"The pValaue is \" + str(pValue) + \" which is significant enough to reject null hypothesis\")\n",
    "    \"\"\"\n",
    "    print(\"Drop VNST due to statistical insignificance\")\n",
    "    data.drop('VNST', axis=1, inplace = True)\n",
    "    \n",
    "    # Seperate the Size feature into Size and Body\n",
    "    tempSize = data['Size'].str.split(' ', expand = True) # Create temp var with split column\n",
    "    data['Size'] = tempSize[0] # Save the temp var back into data\n",
    "    data['Body'] = tempSize[1] # Save the temp var back into data\n",
    "    data['Body'].fillna('CITY', inplace = True) # Assume any other cars are 'City'\n",
    "    \n",
    "    \n",
    "    data.loc[data.Size == 'VAN', 'Body'] = 'Van' # Convert Van into a body type\n",
    "    data.loc[data.Size == 'VAN', 'Body'] =  np.nan # Take van away from size, shouldn't matter once OH is done\n",
    "    \n",
    "\n",
    "    # Replace all non 0, 1 values in IsOnlineSale to 1\n",
    "    maskOnlineSale = data['IsOnlineSale'] != 0 # Any value that isn't 0 will be set to 1\n",
    "    data.loc[maskOnlineSale, 'IsOnlineSale'] = 1 # Set the values to 1\n",
    "    \n",
    "    \n",
    "    # Converting the TimeStamp into Quater\n",
    "    Quarter = [] # Create empty string\n",
    "    for i, _ in enumerate(data.PurchaseTimestamp): # Loop over the entire dataset\n",
    "        # Convert the epoch datetime into the quater and append to list\n",
    "        Quarter.append(pd.Timestamp(data.PurchaseTimestamp.loc[i], unit = 's').quarter)        \n",
    "    data['Quarter'] = Quarter # Create the column with list\n",
    "    data.drop('PurchaseTimestamp', axis=1, inplace = True) # Drop old TimeStamp\n",
    "    \n",
    "    \"\"\" This will take care of any Null values we don't specifically take care of\n",
    "    by replaceing the missing data with data from the same distibution\"\"\"\n",
    "    conv = []\n",
    "    for i in data.columns: # Loop over dataset\n",
    "        if data[i].isna().any() == True: # Check to see if there is a NaN is the feature\n",
    "            dist = data[i].value_counts(normalize=True) # Find the distrabution of the column\n",
    "            missing = data[i].isna() # Find where the NaN are\n",
    "            # Replace the NaNs with values from the same distrabution of the column\n",
    "            data.loc[missing, i] = np.random.choice(dist.index, size=len(data[missing]),p=dist.values)       \n",
    "            conv.append(i)\n",
    "    print(\"Converted all of \" , conv, \"s missing values into the same distrubution\")\n",
    "    \n",
    "    \n",
    "    data[\"MMRAcquisitionAuctionAveragePrice\"].fillna(data[\"MMRAcquisitionAuctionAveragePrice\"].mean(), inplace=True)\n",
    "    data[\"MMRAcquisitionAuctionCleanPrice\"].fillna(data[\"MMRAcquisitionAuctionCleanPrice\"].mean(), inplace=True)\n",
    "    data[\"MMRAcquisitionRetailAveragePrice\"].fillna(data[\"MMRAcquisitionRetailAveragePrice\"].mean(), inplace=True)\n",
    "    data[\"MMRAcquisitonRetailCleanPrice\"].fillna(data[\"MMRAcquisitonRetailCleanPrice\"].mean(), inplace=True)\n",
    "    data[\"MMRCurrentAuctionAveragePrice\"].fillna(data[\"MMRCurrentAuctionAveragePrice\"].mean(), inplace=True)\n",
    "    data[\"MMRCurrentAuctionCleanPrice\"].fillna(data[\"MMRCurrentAuctionCleanPrice\"].mean(), inplace=True)\n",
    "    data[\"MMRCurrentRetailAveragePrice\"].fillna(data[\"MMRCurrentRetailAveragePrice\"].mean(), inplace=True)\n",
    "    data[\"MMRCurrentRetailCleanPrice\"].fillna(data[\"MMRCurrentRetailCleanPrice\"].mean(), inplace=True)\n",
    "    \n",
    "#     print(np.corrcoef(data[\"MMRAcquisitionAuctionAveragePrice\"], data[\"MMRAcquisitionAuctionCleanPrice\"]))\n",
    "#     print(np.corrcoef(data[\"MMRAcquisitionRetailAveragePrice\"], data[\"MMRAcquisitonRetailCleanPrice\"]))\n",
    "#     print(np.corrcoef(data[\"MMRCurrentAuctionAveragePrice\"], data[\"MMRCurrentAuctionCleanPrice\"]))\n",
    "#     print(np.corrcoef(data[\"MMRCurrentRetailAveragePrice\"], data[\"MMRCurrentRetailCleanPrice\"])) \n",
    "    \n",
    "    \"\"\" Create the new columns \"\"\"\n",
    "    data[\"AcquisitionAuctionprice\"] = CreateAveragedColumn(data[\"MMRAcquisitionAuctionAveragePrice\"], data[\"MMRAcquisitionAuctionCleanPrice\"])\n",
    "    data[\"AcquisitionRetailPrice\"] = CreateAveragedColumn(data[\"MMRAcquisitionRetailAveragePrice\"], data[\"MMRAcquisitonRetailCleanPrice\"])\n",
    "    data[\"MMRCurrentAuctionPrice\"] = CreateAveragedColumn(data[\"MMRCurrentAuctionAveragePrice\"], data[\"MMRCurrentAuctionCleanPrice\"])\n",
    "    data[\"MMRCurrentRetailPrice\"] = CreateAveragedColumn(data[\"MMRCurrentRetailAveragePrice\"], data[\"MMRCurrentRetailCleanPrice\"])\n",
    "    \n",
    "    data.drop('MMRAcquisitionAuctionAveragePrice', inplace = True, axis = 1)\n",
    "    data.drop('MMRAcquisitionAuctionCleanPrice', inplace = True, axis = 1)\n",
    "    data.drop('MMRAcquisitonRetailCleanPrice', inplace = True, axis = 1)\n",
    "    data.drop('MMRCurrentAuctionAveragePrice', inplace = True, axis = 1)\n",
    "    data.drop('MMRCurrentAuctionCleanPrice', inplace = True, axis = 1)\n",
    "    data.drop('MMRCurrentRetailAveragePrice', inplace = True, axis = 1)\n",
    "    data.drop('MMRCurrentRetailCleanPrice', inplace = True, axis = 1)\n",
    "    data.drop('MMRAcquisitionRetailAveragePrice', inplace = True, axis = 1)\n",
    "    \n",
    "    \n",
    "     # Now do a box plots\n",
    "    data[\"AcquisitionAuctionprice\"].hist()\n",
    "    data[\"AcquisitionRetailPrice\"].hist()\n",
    "    data[\"MMRCurrentAuctionPrice\"].hist()\n",
    "    data[\"MMRCurrentRetailPrice\"].hist()\n",
    "    \n",
    "    ## Identify and Remove outiers\n",
    "    outliers = (IdentifyOutliers(data[\"AcquisitionAuctionprice\"]) and\n",
    "                IdentifyOutliers(data[\"AcquisitionRetailPrice\"]) and\n",
    "                IdentifyOutliers(data[\"MMRCurrentAuctionPrice\"]) and\n",
    "                IdentifyOutliers(data[\"MMRCurrentRetailPrice\"]))\n",
    "    \n",
    "    # Select lines that aren't outliers\n",
    "    num_lines =  len(data[\"MMRCurrentRetailPrice\"])\n",
    "    data = data[[not i for i in outliers]]\n",
    "    print (\"Lines Removed: \", num_lines - len(data[\"MMRCurrentRetailPrice\"]))\n",
    "    \n",
    "    \n",
    "    \"\"\" This should be the last thing done \"\"\"\n",
    "    # Convert all categorical variables into one hot representations\n",
    "    \n",
    "    print(\"The number of features before one hot encoding is \" + str(data.shape[1]))\n",
    "    data_OH = pd.get_dummies(data, columns = ['Auction', 'Make', 'Color', 'VehYear', \n",
    "                                              'Nationality', 'Size', 'Body', 'TopThreeAmericanName', \n",
    "                                              'WheelType', 'Quarter'])\n",
    "    print(\"The number of features after one hot encoding is \" + str(data_OH.shape[1]))\n",
    "    \n",
    "    return data, data_OH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "## Q1\n",
    "The proportion of cars that are kick is 12.949...%\n",
    "Detailed below\n",
    "## Q2\n",
    "Yes. Details below\n",
    "Treat '?' and '#VALUE' as NaN\n",
    "PRIMEUNIT and AUCGUART were dropped due to only having a small amount of data\n",
    "WheelTypeID and PurchaseDate were duplicates of other features and were dropped\n",
    "ForSale was dropped because of the data skew\n",
    "In TopThreeAmericanName Hyundai and Jeep were renamed into HYUNDAI and JEEP\n",
    "All other cols had thwere strings converted to uppercase\n",
    "In nationality USA was replaced to AMERICA\n",
    "Transmission was renamed to Auto and Auto was set to 1, Manual set to 0\n",
    "In Color, NOT AVIL was changed to NaN\n",
    "Dropped VNST due to statisical insignificance\n",
    "Size was split inot Size and Body and Van was placed into the Body feature\n",
    "Any non-zero values in IsOnlineSale was set to 1\n",
    "Changed PurchaseTimestamp into days of week\n",
    "Replace any missing values not specifically taken care of previously to a value choosen randomly from the distibution of the column\n",
    "## Q3\n",
    "\n",
    "## Q4\n",
    "\n",
    "## Q5\n",
    "Used a train test split with a 20% test set. Using CV allows us to have a smaller test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categoricalPlot(cat, cat2 = 'IsBadBuy'): # Cat is the carigorical as a string i.e 'Size'\n",
    "    pd.crosstab(data[cat],data[cat2]).plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateAveragedColumn (A, B):\n",
    "    C = [np.nan] * len( A )\n",
    "    for i in range(len(A)):\n",
    "        C[i] = (A[i] + B[i])/2\n",
    "    \n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IdentifyOutliers (column):\n",
    "    data_described = column.describe()\n",
    "    Q1 = data_described[\"25%\"]\n",
    "    Q3 = data_described[\"75%\"]\n",
    "    IQR = Q3-Q1\n",
    "    lowerLimit = Q1 - 1.5 * IQR\n",
    "    upperLimit = Q3 + 1.5 * IQR\n",
    "    output = [np.nan] * len( column )\n",
    "    for i in range(len(column)):\n",
    "        value = column[i]\n",
    "        if value < lowerLimit or value > upperLimit:\n",
    "            output[i] = True\n",
    "        else:\n",
    "            output[i] = False\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_feature_importance(dm_model, feature_names, n_to_display=5):\n",
    "    # grab feature importances from the model\n",
    "    importances = dm_model.feature_importances_\n",
    "    \n",
    "    # sort them out in descending order\n",
    "    indices = np.argsort(importances)\n",
    "    indices = np.flip(indices, axis=0)\n",
    "\n",
    "    # limit to 20 features, you can leave this out to print out everything\n",
    "    indices = indices[:n_to_display]\n",
    "\n",
    "    for i in indices:\n",
    "        print(feature_names[i], ':', importances[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.read_csv(\"Kick.csv\", # Read the csv into a DF\n",
    "                   index_col = 'PurchaseID', \n",
    "                   na_values = ('?',  '#VALUE!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, data_OH = PreProcessing(data_raw) # PreProcess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = data[\"IsBadBuy\"].value_counts()\n",
    "total = data[\"IsBadBuy\"].count()\n",
    "    \n",
    "# calculate the percentage\n",
    "# kick is where IsBadBuy == 1\n",
    "kickPersentage = counts[1] / total * 100\n",
    "print (\"Questin 1.1 :\")\n",
    "print( kickPersentage, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target/input split\n",
    "y = data_OH['IsBadBuy']\n",
    "X = data_OH.drop(['IsBadBuy'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mat = X.values # Canvert X into a matrix\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mat, y, test_size=0.2, stratify=y, random_state=rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelDT = DecisionTreeClassifier(random_state=rs) # Define the model\n",
    "modelDT.fit(X_train, y_train) # Fit with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train accuracy:\", modelDT.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", modelDT.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predDT = modelDT.predict(X_test) # Predict the data\n",
    "print(classification_report(y_test, y_predDT)) # Print the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab feature importances from the model and feature name from the original X\n",
    "feature_names = X.columns\n",
    "\n",
    "analyse_feature_importance(modelDT, feature_names, n_to_display=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to find the root node\n",
    "def getTreeInfo(model, feature_names):\n",
    "    features  = [feature_names[i] for i in model.tree_.feature]\n",
    "    print(\"Root feature is \", features[0])\n",
    "    print(\"Competing featues are \", features[1], 'and ', features[2])\n",
    "    print(\"Number of nodes is \", model.tree_.node_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'getTreeInfo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-5d7bae3e5268>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgetTreeInfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodelDT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Call the root function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'getTreeInfo' is not defined"
     ]
    }
   ],
   "source": [
    "getTreeInfo(modelDT, feature_names) # Call the root function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Create a graph of the tree\\ndef DTreeGraph(model, colNames):\\n    dotfile = StringIO()\\n    export_graphviz(model, out_file=dotfile, feature_names=colNames)\\n    graph = pydot.graph_from_dot_data(dotfile.getvalue())\\n    graph[0].write_png('grpah.png') # saved in the following file\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Create a graph of the tree\n",
    "def DTreeGraph(model, colNames):\n",
    "    dotfile = StringIO()\n",
    "    export_graphviz(model, out_file=dotfile, feature_names=colNames)\n",
    "    graph = pydot.graph_from_dot_data(dotfile.getvalue())\n",
    "    graph[0].write_png('grpah.png') # saved in the following file\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This is take a while\n",
    "DTreeGraph(modelDT, feature_names)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrain with a larger minimum number of samples required to split an internal node\n",
    "test_scoreDT = []\n",
    "train_scoreDT = []\n",
    "\n",
    "range_ = range(35, 45)\n",
    "\n",
    "# check the model performance for max depth from 2-20\n",
    "for min_samples_split in range_:\n",
    "    modelDT = DecisionTreeClassifier(min_samples_split=min_samples_split, random_state=rs)\n",
    "    modelDT.fit(X_train, y_train)\n",
    "    \n",
    "    test_scoreDT.append(modelDT.score(X_test, y_test))\n",
    "    train_scoreDT.append(modelDT.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range_, train_scoreDT, 'b', range_, test_scoreDT, 'r')\n",
    "plt.xlabel('max_depth\\nBlue = training acc. Red = test acc.')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_feature_importance(modelDT, feature_names, n_to_display=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_features = data[['VehOdo', 'VehBCost', 'MMRCurrentAuctionAveragePrice', 'MMRCurrentRetailRatio', 'MMRCurrentRetailCleanPrice']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getTreeInfo(modelDT, feature_names) # Call the root function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2.1\n",
    "\n",
    "## Q 1.a\n",
    "\n",
    "## Q 1.b\n",
    "\n",
    "## Q 1.c\n",
    "\n",
    "## Q 1.d\n",
    "\n",
    "## Q 1.e\n",
    "\n",
    "## Q 1.f\n",
    "\n",
    "## Q 1.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a grid search over the best hyperparameters\n",
    "params = {'criterion': ['gini', 'entropy'], # What criterion to check\n",
    "          'max_depth': range(2, 7), # Check the depth, use the graph generated above\n",
    "          'min_samples_leaf': range(5, 26, 5)} # Define the min sample leafs\n",
    "\n",
    "cvDT = GridSearchCV(param_grid=params, estimator=DecisionTreeClassifier(random_state=rs), cv=10) # Define the model\n",
    "cvDT.fit(X_train, y_train) # Fit the data to the model\n",
    "y_predDT = cvDT.predict(X_test) # test the best model\n",
    "\n",
    "print(\"Using grid search the accuracy is\")\n",
    "print(\"Train accuracy:\", cvDT.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", cvDT.score(X_test, y_test))\n",
    "print(classification_report(y_test, y_predDT))\n",
    "\n",
    "# print parameters of the best model\n",
    "print(cvDT.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use these parameters to refine the model\n",
    "params = {'criterion': ['gini', 'entropy'],\n",
    "          'max_depth': range(2, cv.best_params_['max_depth']+2),\n",
    "          'min_samples_leaf': range(cv.best_params_['min_samples_leaf']-4, \n",
    "                                    cv.best_params_['min_samples_leaf']+5)}\n",
    "\n",
    "cvDT = GridSearchCV(param_grid=params, estimator=DecisionTreeClassifier(criterion= cv.best_params_['criterion'], random_state=rs), cv=10)\n",
    "cvDT.fit(X_train, y_train)\n",
    "\n",
    "print(\"Using the refinded parameters\")\n",
    "print(\"Train accuracy:\", cvDT.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", cvDT.score(X_test, y_test))\n",
    "\n",
    "# test the best model\n",
    "y_predDT = cvDT.predict(X_test)\n",
    "print(classification_report(y_test, y_predDT))\n",
    "\n",
    "# print parameters of the best model\n",
    "print(cvDT.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2.2\n",
    "\n",
    "## Q 2.a\n",
    "\n",
    "## Q 2.b\n",
    "\n",
    "## Q 2.c\n",
    "\n",
    "## Q 2.d\n",
    "\n",
    "## Q 2.e\n",
    "\n",
    "## Q 2.f\n",
    "\n",
    "## Q 2.g\n",
    "\n",
    "# Task 2.3\n",
    "\n",
    "# Task 2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "scaler = StandardScaler() # Use the standard scaler\n",
    "print(\"The statisics of the data before scaling\")\n",
    "print(\"Before scaling\\n-------------\")\n",
    "for i in range(5):\n",
    "    col = X_train[:,i]\n",
    "    print(\"Variable #{}: min {}, max {}, mean {:.2f} and std dev {:.2f}\".\n",
    "          format(i, min(col), max(col), np.mean(col), np.std(col)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaler.fit_transform(X_train, y_train) # Scale the data\n",
    "X_test = scaler.transform(X_test) # and the training data\n",
    "print(\"After scaling\\n-------------\")\n",
    "for i in range(5):\n",
    "    col = X_train[:,i]\n",
    "    print(\"Variable #{}: min {}, max {}, mean {:.2f} and std dev {:.2f}\".\n",
    "          format(i, min(col), max(col), np.mean(col), np.std(col)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_mat = DT_features.values\n",
    "DT_train, DT_test, _, _ = train_test_split(DT_mat, y, test_size=0.2, stratify=y, random_state=rs)\n",
    "DT_train = scaler.fit_transform(DT_train) # Scale the data\n",
    "DT_test = scaler.transform(DT_test) # and the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelLG = LogisticRegression(random_state=rs) # Define the model\n",
    "modelLG.fit(X_train, y_train) # fit it to training data\n",
    "y_predLG = modelLG.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and test accuracy\n",
    "print(\"Train accuracy:\", modelLG.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", modelLG.score(X_test, y_test))\n",
    "# classification report on test data\n",
    "print(classification_report(y_test, y_predLG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefLG = modelLG.coef_[0]\n",
    "\n",
    "# limit to 20 features, you can comment the following line to print out everything\n",
    "coefLG = coefLG[:5]\n",
    "\n",
    "for i in range(len(coefLG)):\n",
    "    print(feature_names[i], ':', coefLG[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GCV to find the optimal parameters\n",
    "# grid search CV\n",
    "params = {'C': [pow(10, x) for x in range(-6, 4)]}\n",
    "\n",
    "# use all cores to tune logistic regression with C parameter\n",
    "cvLG = GridSearchCV(param_grid=params, estimator=LogisticRegression(random_state=rs), cv=10, n_jobs=-1)\n",
    "cvLG.fit(X_train, y_train)\n",
    "\n",
    "# test the best model\n",
    "print(\"Train accuracy:\", cvLG.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", cvLG.score(X_test, y_test))\n",
    "\n",
    "y_predLG = cvLG.predict(X_test)\n",
    "print(classification_report(y_test, y_predLG))\n",
    "\n",
    "# print parameters of the best model\n",
    "print(cvLG.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best paramaters from previous CV to get closer to optimum values\n",
    "# grid search CV\n",
    "params = {'C': [pow(10, x) for x in range(-4, -2)]}\n",
    "\n",
    "# use all cores to tune logistic regression with C parameter\n",
    "cvLG = GridSearchCV(param_grid=params, estimator=LogisticRegression(random_state=rs), cv=10, n_jobs=-1)\n",
    "cvLG.fit(X_train, y_train)\n",
    "\n",
    "# test the best model\n",
    "print(\"Train accuracy:\", cvLG.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", cvLG.score(X_test, y_test))\n",
    "\n",
    "y_predLG = cvLG.predict(X_test)\n",
    "print(classification_report(y_test, y_predLG))\n",
    "\n",
    "# print parameters of the best model\n",
    "print(cvLG.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3.1\n",
    "\n",
    "# Task 3.2\n",
    "\n",
    "# Task 3.3\n",
    "\n",
    "## Q3.h\n",
    "\n",
    "## Q3.i\n",
    "\n",
    "## Q3.j\n",
    "\n",
    "## Q3.k\n",
    "\n",
    "## Q3.l\n",
    "\n",
    "## Q3.m\n",
    "\n",
    "## Q3.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive feature elimination\n",
    "rfe = RFECV(estimator = LogisticRegression(random_state=rs), cv=10) # fit the model\n",
    "rfe.fit(X_train, y_train) # run the RFECV\n",
    "\n",
    "# comparing how many variables before and after\n",
    "print(\"Original feature set\", X_train.shape[1])\n",
    "print(\"Number of features after elimination\", rfe.n_features_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sel = rfe.transform(X_train)\n",
    "X_test_sel = rfe.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search CV\n",
    "params = {'C': [pow(10, x) for x in range(-4, 1)]}\n",
    "\n",
    "cvRFE = GridSearchCV(param_grid=params, estimator=LogisticRegression(random_state=rs), cv=10, n_jobs=-1)\n",
    "cvRFE.fit(X_train_sel, y_train)\n",
    "\n",
    "# test the best model\n",
    "print(\"Train accuracy:\", cvRFE.score(X_train_sel, y_train))\n",
    "print(\"Test accuracy:\", cvRFE.score(X_test_sel, y_test))\n",
    "\n",
    "y_predRFE = cvRFE.predict(X_test_sel)\n",
    "print(classification_report(y_test, y_predRFE))\n",
    "\n",
    "# print parameters of the best model\n",
    "print(cvRFE.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3.4 \n",
    "\n",
    "## Q4.a\n",
    "\n",
    "## Q4.b\n",
    "\n",
    "## Q4.c\n",
    "\n",
    "## Q4.d\n",
    "\n",
    "## Q4.e\n",
    "\n",
    "## Q4.f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelMLP = MLPClassifier(random_state=rs) # Define the model\n",
    "modelMLP.fit(X_train, y_train) # Fit it\n",
    "\n",
    "print(\"Train accuracy:\", modelMLP.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", modelMLP.score(X_test, y_test))\n",
    "\n",
    "y_predMLP = modelMLP.predict(X_test)\n",
    "print(classification_report(y_test, y_predMLP))\n",
    "\n",
    "print(modelMLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GCV and change the no. of hidden nodes\n",
    "params = {'hidden_layer_sizes': [(x,) for x in range(2, 7)], 'alpha': [pow(10, x) for x in range(-2, -6)]}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=MLPClassifier(random_state=rs), cv=10, n_jobs=-1)\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Train accuracy:\", cv.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", cv.score(X_test, y_test))\n",
    "\n",
    "y_pred = cv.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step = int((X_train_rfe.shape[1] + 5)/5);\n",
    "params = {'hidden_layer_sizes': [(3,), (5,), (7,), (9,)], 'alpha': [pow(10, x) for x in range(-3, -6)]}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=MLPClassifier(random_state=rs), cv=10, n_jobs=-1)\n",
    "cv.fit(X_train_sel, y_train)\n",
    "\n",
    "print(\"Train accuracy:\", cv.score(X_train_sel, y_train))\n",
    "print(\"Test accuracy:\", cv.score(X_test_sel, y_test))\n",
    "\n",
    "y_pred = cv.predict(X_test_sel)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the features found from the decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Change some of these numbers\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# MLP with decision tree features\n",
    "modelMLP = MLPClassifier(hidden_layer_sizes = (3,), alpha = 0.0001, random_state=rs) # Define the model\n",
    "modelMLP.fit(DT_train, y_train) # Fit it\n",
    "\n",
    "print(\"Train accuracy:\", modelMLP.score(DT_train, y_train))\n",
    "print(\"Test accuracy:\", modelMLP.score(DT_test, y_test))\n",
    "\n",
    "y_predMLP = modelMLP.predict(DT_test)\n",
    "print(classification_report(y_test, y_predMLP))\n",
    "\n",
    "print(modelMLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Change some of these numbers\n",
    "\"\"\"\n",
    "\n",
    "# MLP with rfe features\n",
    "modelMLP = MLPClassifier(hidden_layer_sizes = (3,), alpha = 0.0001, random_state=rs) # Define the model\n",
    "modelMLP.fit(X_train_sel, y_train) # Fit it\n",
    "\n",
    "print(\"Train accuracy:\", modelMLP.score(X_train_sel, y_train))\n",
    "print(\"Test accuracy:\", modelMLP.score(X_test_sel, y_test))\n",
    "\n",
    "y_predMLP = modelMLP.predict(X_test_sel)\n",
    "print(classification_report(y_test, y_predMLP))\n",
    "\n",
    "print(modelMLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrain models with selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelDT = DecisionTreeClassifier(random_state=rs) # Define the model\n",
    "modelDT.fit(X_train_sel, y_train) # Fit with data\n",
    "\n",
    "\n",
    "modelMLP = MLPClassifier(random_state=rs) # Define the model\n",
    "modelMLP.fit(X_train_sel, y_train) # Fit it\n",
    "\n",
    "modelLG = LogisticRegression(random_state=rs) # Define the model\n",
    "modelLG.fit(X_train_sel, y_train) # fit it to training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the models\n",
    "\n",
    "# ROC AUC\n",
    "y_pred_proba_dt = modelDT.predict_proba(X_test_sel)\n",
    "y_pred_proba_log_reg = modelLG.predict_proba(X_test_sel)\n",
    "y_pred_proba_nn = modelMLP.predict_proba(X_test_sel)\n",
    "\n",
    "roc_index_dt = roc_auc_score(y_test, y_pred_proba_dt[:, 1])\n",
    "roc_index_log_reg = roc_auc_score(y_test, y_pred_proba_log_reg[:, 1])\n",
    "roc_index_nn = roc_auc_score(y_test, y_pred_proba_nn[:, 1])\n",
    "\n",
    "print(\"ROC index on test for DT:\", roc_index_dt)\n",
    "print(\"ROC index on test for logistic regression:\", roc_index_log_reg)\n",
    "print(\"ROC index on test for NN:\", roc_index_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_dt, tpr_dt, thresholds_dt = roc_curve(y_test, y_pred_proba_dt[:,1])\n",
    "fpr_log_reg, tpr_log_reg, thresholds_log_reg = roc_curve(y_test, y_pred_proba_log_reg[:,1])\n",
    "fpr_nn, tpr_nn, thresholds_nn = roc_curve(y_test, y_pred_proba_nn[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr_dt, tpr_dt, label='ROC Curve for DT {:.3f}'.format(roc_index_dt), color='red', lw=0.5)\n",
    "plt.plot(fpr_log_reg, tpr_log_reg, label='ROC Curve for Log reg {:.3f}'.format(roc_index_log_reg), color='green', lw=0.5)\n",
    "plt.plot(fpr_nn, tpr_nn, label='ROC Curve for NN {:.3f}'.format(roc_index_nn), color='darkorange', lw=0.5)\n",
    "\n",
    "# plt.plot(fpr[2], tpr[2], color='darkorange',\n",
    "#          lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=0.5, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4\n",
    "## Q1.a\n",
    "\n",
    "## Q1.b\n",
    "\n",
    "## Q1.c\n",
    "\n",
    "## Q1.d\n",
    "\n",
    "## Q1.e\n",
    "\n",
    "## Q2.a\n",
    "\n",
    "## Q2.b\n",
    "\n",
    "## Q2.c\n",
    "\n",
    "## Q2.d\n",
    "\n",
    "## Q2.e\n",
    "\n",
    "## Q3.a\n",
    "\n",
    "## Q3.b\n",
    "\n",
    "## Q3.c\n",
    "\n",
    "## Q3.d\n",
    "\n",
    "## Q3.e\n",
    "\n",
    "## Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the classifier with 3 different estimators\n",
    "voting = VotingClassifier(estimators=[('dt', modelDT), ('lr', modelLG), ('nn', modelMLP)], voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the voting classifier to training data\n",
    "voting.fit(X_train, y_train)\n",
    "\n",
    "# evaluate train and test accuracy\n",
    "print(\"Ensemble train accuracy:\", voting.score(X_train, y_train))\n",
    "print(\"Ensemble test accuracy:\", voting.score(X_test, y_test))\n",
    "\n",
    "# evaluate ROC auc score\n",
    "y_pred_proba_ensemble = voting.predict_proba(X_test)\n",
    "roc_index_ensemble = roc_auc_score(y_test, y_pred_proba_ensemble[:, 1])\n",
    "print(\"ROC score of voting classifier:\", roc_index_ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5\n",
    "\n",
    "## Q1.a\n",
    "\n",
    "## Q2.a\n",
    "\n",
    "## Q2.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6\n",
    "\n",
    "## Q1\n",
    "\n",
    "## Q2\n",
    "\n",
    "## Q3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
