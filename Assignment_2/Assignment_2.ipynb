{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: apyori in c:\\programdata\\anaconda3\\lib\\site-packages (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "### Installs ###\n",
    "!pip install apyori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORTS ###\n",
    "#Clustering\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from apyori import apriori\n",
    "\n",
    "# String Processing\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUNCTIONS ###\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Children of hierarchical clustering\n",
    "    children = model.children_\n",
    "\n",
    "    # Distances between each pair of children\n",
    "    # Since we don't have this information, we can use a uniform one for plotting\n",
    "    distance = np.arange(children.shape[0])\n",
    "\n",
    "    # The number of observations contained in each cluster level\n",
    "    no_of_observations = np.arange(2, children.shape[0]+2)\n",
    "\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "    linkage_matrix = np.column_stack([children, distance, no_of_observations]).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)\n",
    "    \n",
    "def convert_apriori_results_to_pandas_df(results):\n",
    "    rules = []\n",
    "    \n",
    "    for rule_set in results:\n",
    "        for rule in rule_set.ordered_statistics:\n",
    "            # items_base = left side of rules, items_add = right side\n",
    "            # support, confidence and lift for respective rules\n",
    "            rules.append([','.join(rule.items_base), ','.join(rule.items_add),\n",
    "                         rule_set.support, rule.confidence, rule.lift]) \n",
    "    \n",
    "    # typecast it to pandas df\n",
    "    return pd.DataFrame(rules, columns=['Left_side', 'Right_side', 'Support', 'Confidence', 'Lift']) \n",
    "\n",
    "def lemmatize(token, tag):\n",
    "    tag = {\n",
    "        'N': wn.NOUN,\n",
    "        'V': wn.VERB,\n",
    "        'R': wn.ADV,\n",
    "        'J': wn.ADJ\n",
    "    }.get(tag[0], wn.NOUN)\n",
    "\n",
    "    return lemmatizer.lemmatize(token, tag)\n",
    "\n",
    "def cab_tokenizer(document):\n",
    "    # initialize token list\n",
    "    tokens = []\n",
    "    \n",
    "    # split the document into sentences\n",
    "    for sent in sent_tokenize(document):\n",
    "        # split the document into tokens and then create part of speech tag for each token\n",
    "        for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "            # preprocess and remove unnecessary characters\n",
    "            token = token.lower()\n",
    "            token = token.strip()\n",
    "            token = token.strip('_')\n",
    "            token = token.strip('*')\n",
    "\n",
    "            # If stopword, ignore token and continue\n",
    "            if token in stopwords:\n",
    "                continue\n",
    "\n",
    "            # If punctuation, ignore token and continue\n",
    "            if all(char in punct for char in token):\n",
    "                continue\n",
    "\n",
    "            # Lemmatize the token and add back to the tokens list\n",
    "            lemma = lemmatize(token, tag)\n",
    "            tokens.append(lemma)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# function to visualise text cluster. Useful for the assignment too :)\n",
    "def visualise_text_cluster(n_clusters, cluster_centers, terms, num_word = 5):\n",
    "    # -- Params --\n",
    "    # cluster_centers: cluster centers of fitted/trained KMeans/other centroid-based clustering\n",
    "    # terms: terms used for clustering\n",
    "    # num_word: number of terms to show per cluster. Change as you please.\n",
    "    \n",
    "    # find features/terms closest to centroids\n",
    "    ordered_centroids = cluster_centers.argsort()[:, ::-1]\n",
    "    \n",
    "    for cluster in range(n_clusters):\n",
    "        print(\"Top terms for cluster {}:\".format(cluster), end=\" \")\n",
    "        for term_idx in ordered_centroids[cluster, :5]:\n",
    "            print(terms[term_idx], end=', ')\n",
    "            \n",
    "# creating tf-idf terms - a bit slow, do it occasionaly\n",
    "def calculate_tf_idf_terms(document_col):\n",
    "    # Param - document_col: collection of raw document text that you want to analyse\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "    # use count vectorizer to find TF and DF of each term\n",
    "    count_vec = CountVectorizer(tokenizer=cab_tokenizer, ngram_range=(1,2))\n",
    "    X_count = count_vec.fit_transform(df['Text'])\n",
    "    \n",
    "    # create list of terms and their tf and df\n",
    "    terms = [{'term': t, 'idx': count_vec.vocabulary_[t],\n",
    "              'tf': X_count[:, count_vec.vocabulary_[t]].sum(),\n",
    "              'df': X_count[:, count_vec.vocabulary_[t]].count_nonzero()}\n",
    "             for t in count_vec.vocabulary_]\n",
    "    \n",
    "    return terms\n",
    "\n",
    "# visualisation of ZIPF law\n",
    "def visualise_zipf(terms, itr_step = 50):\n",
    "    from scipy.spatial.distance import euclidean\n",
    "    from math import sqrt\n",
    "    \n",
    "    # --- Param ---\n",
    "    # terms: collection of terms dictionary from calculate_tf_idf_terms function\n",
    "    # itr_step: used to control how many terms that you want to plot. Num of terms to plot = N terms / itr_step\n",
    "    \n",
    "    # sort terms by its frequency\n",
    "    terms.sort(key=lambda x: (x['tf'], x['df']), reverse=True)\n",
    "    \n",
    "    # select a few of the terms for plotting purpose\n",
    "    sel_terms = [terms[i] for i in range(0, len(terms), itr_step)]\n",
    "    labels = [term['term'] for term in sel_terms]\n",
    "    \n",
    "    # plot term frequency ranking vs its DF\n",
    "    plt.plot(range(len(sel_terms)), [x['df'] for x in sel_terms])\n",
    "    plt.xlabel('Term frequency ranking')\n",
    "    plt.ylabel('Document frequency')\n",
    "    \n",
    "    max_x = len(sel_terms)\n",
    "    max_y = max([x['df'] for x in sel_terms])\n",
    "    \n",
    "    # annotate the points\n",
    "    prev_x, prev_y = 0, 0\n",
    "    for label, x, y in zip(labels,range(len(sel_terms)), [x['df'] for x in sel_terms]):\n",
    "        # calculate the relative distance between labels to increase visibility\n",
    "        x_dist = (abs(x - prev_x) / float(max_x)) ** 2\n",
    "        y_dist = (abs(y - prev_y) / float(max_y)) ** 2\n",
    "        scaled_dist = sqrt(x_dist + y_dist)\n",
    "        \n",
    "        if (scaled_dist > 0.1):\n",
    "            plt.text(x+2, y+2, label, {'ha': 'left', 'va': 'bottom'}, rotation=30)\n",
    "            prev_x, prev_y = x, y\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PRE-PROCESSING ###\n",
    "def _PerpData (df):\n",
    "    \n",
    "    # Get the names of the columns\n",
    "    _names = list(df)\n",
    "    \n",
    "    # loop through the data and change all null to np.nan\n",
    "    for _name in _names:\n",
    "        df[_name] = df[_name].replace('', np.nan)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAIN ###\n",
    "# Read the Data\n",
    "data = pd.read_csv('census2000.csv', na_filter=False)\n",
    "data = _PerpData(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Clustering</h1>\n",
    "<h3>K-Means<h3/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### KMeans ###\n",
    "# take 3 variables and drop the rest\n",
    "data_kmeans = data[['MedHHInc', 'MeanHHSz', 'RegDens']]\n",
    "\n",
    "# convert df2 to matrix\n",
    "X = data_kmeans.as_matrix()\n",
    "\n",
    "# scaling\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# random state, we will use 42 instead of 10 for a change\n",
    "rs = 42\n",
    "\n",
    "# set the random state. different random state seeds might result in different centroids locations\n",
    "model = KMeans(n_clusters=3, random_state=rs)\n",
    "model.fit(X)\n",
    "\n",
    "# sum of intra-cluster distances\n",
    "print(\"Sum of intra-cluster distance:\", model.inertia_)\n",
    "\n",
    "print(\"Centroid locations:\")\n",
    "for centroid in model.cluster_centers_:\n",
    "    print(centroid)\n",
    "    \n",
    "# set a different n_clusters\n",
    "model = KMeans(n_clusters=8, random_state=rs)\n",
    "model.fit(X)\n",
    "\n",
    "# sum of intra-cluster distances\n",
    "print(\"Sum of intra-cluster distance:\", model.inertia_)\n",
    "\n",
    "print(\"Centroid locations:\")\n",
    "for centroid in model.cluster_centers_:\n",
    "    print(centroid)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Agglomerative Clustering | (Alternative to Kmeans Clustering)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_model = AgglomerativeClustering(n_clusters=3)\n",
    "agg_model.fit(X[:50]) # subset of X, only 50 data points\n",
    "\n",
    "plot_dendrogram(agg_model, labels=agg_model.labels_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Understanding and Visualising a Clustering Model</h3>\n",
    "<h4>Method 1: Pair Plots</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=3, random_state=rs).fit(X)\n",
    "\n",
    "# assign cluster ID to each record in X\n",
    "# Ignore the warning, does not apply to our case here\n",
    "y = model.predict(X)\n",
    "data_kmeans['Cluster_ID'] = y\n",
    "\n",
    "# how many records are in each cluster\n",
    "print(\"Cluster membership\")\n",
    "print(data_kmeans['Cluster_ID'].value_counts())\n",
    "\n",
    "# pairplot the cluster distribution.\n",
    "cluster_g = sns.pairplot(data_kmeans, hue='Cluster_ID')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Method 2: Distribution Plots</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the column and bin size. Increase bin size to be more specific, but 20 is more than enough\n",
    "cols = ['MedHHInc', 'MeanHHSz', 'RegDens']\n",
    "n_bins = 20\n",
    "\n",
    "# inspecting cluster 0 and 1\n",
    "clusters_to_inspect = [0,1]\n",
    "\n",
    "for cluster in clusters_to_inspect:\n",
    "    # inspecting cluster 0b\n",
    "    print(\"Distribution for cluster {}\".format(cluster))\n",
    "\n",
    "    # create subplots\n",
    "    fig, ax = plt.subplots(nrows=3)\n",
    "    ax[0].set_title(\"Cluster {}\".format(cluster))\n",
    "\n",
    "    for j, col in enumerate(cols):\n",
    "        # create the bins\n",
    "        bins = np.linspace(min(data_kmeans[col]), max(data_kmeans[col]), 20)\n",
    "        # plot distribution of the cluster using histogram\n",
    "        sns.distplot(data_kmeans[data_kmeans['Cluster_ID'] == cluster][col], bins=bins, ax=ax[j], norm_hist=True)\n",
    "        # plot the normal distribution with a black line\n",
    "        sns.distplot(data_kmeans[col], bins=bins, ax=ax[j], hist=False, color=\"k\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Determine K</h3>\n",
    "<h4>Method 1: Elbow method </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list to save the clusters and cost\n",
    "clusters = []\n",
    "inertia_vals = []\n",
    "\n",
    "# this whole process should take a while\n",
    "for k in range(2, 15, 2):\n",
    "    # train clustering with the specified K\n",
    "    model = KMeans(n_clusters=k, random_state=rs, n_jobs=10)\n",
    "    model.fit(X)\n",
    "    \n",
    "    # append model to cluster list\n",
    "    clusters.append(model)\n",
    "    inertia_vals.append(model.inertia_)\n",
    "\n",
    "# plot the inertia vs K values\n",
    "plt.plot(range(2,15,2), inertia_vals, marker='*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Method 2: Silhoette Score </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clusters[1])\n",
    "print(\"Silhouette score for k=4\", silhouette_score(X, clusters[1].predict(X)))\n",
    "\n",
    "print(clusters[2])\n",
    "print(\"Silhouette score for k=6\", silhouette_score(X, clusters[2].predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Performing Association Mining</h2>\n",
    "<h3>Apriori algorithm</h3>\n",
    "<h4>Create Transactions</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by account, then list all services\n",
    "transactions = df.groupby(['GROUP_BY_FIELD'])['PROPERTY_FIELD'].apply(list)\n",
    "\n",
    "print(transactions.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><u>Possible Parameters</u></b>\n",
    "\n",
    "1. `transactions`: list of list of items in transactions (eg. [['A', 'B'], ['B', 'C']]).\n",
    "2. `min_support`: Minimum support of relations in float percentage. It specifies a minimum level of support to claim that items are associated (i.e. they occur together in the dataset). Default 0.1.\n",
    "3. `min_confidence`: Minimum confidence of relations in float percentage. Default 0.0.\n",
    "4. `min_lift`: Minimum lift of relations in float percentage. Default 0.0.\n",
    "5. `max_length`: Max length of the relations. Default None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type cast the transactions from pandas into normal list format and run apriori\n",
    "transaction_list = list(transactions)\n",
    "results = list(apriori(transaction_list, min_support=0.05))\n",
    "\n",
    "# print first 5 rules\n",
    "print(results[:5])\n",
    "\n",
    "result_df = convert_apriori_results_to_pandas_df(results)\n",
    "\n",
    "print(result_df.head(20))\n",
    "\n",
    "## Get the most appropriate variables ##\n",
    "# sort all acquired rules descending by lift\n",
    "result_df = result_df.sort_values(by='Lift', ascending=False)\n",
    "print(result_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Text Mining</h2>\n",
    "<h3>Data reading</h3>\n",
    "<h4>Loading Text</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "df = pd.read_json('datasets/Federalistpapers.json')\n",
    "\n",
    "# random state\n",
    "rs = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Data Exploration</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as usual, explore the dataset\n",
    "df.info()\n",
    "\n",
    "# print out the first 200 characters of the first row of text column\n",
    "print(df.get_value(index=0, col='Text')[:200])\n",
    "\n",
    "# average length of text column\n",
    "print(df['Text'].apply(lambda x: len(x)).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data Prepocessing</h3>\n",
    "<h4>Bag Of Words</h4>\n",
    "\n",
    "1. **Lowercase**: cast each word into its lowercased version. Ensure differently capitalised words are treated as the same word.\n",
    "2. **Punctuation** removal: remove all punctuation marks. We are only interested at the words.\n",
    "3. **Part of speech** filtering: keep tokens with certain part-of-speech only. In particular, here we are interested in adjectives, adverbs, nouns and verbs.\n",
    "4. **Lemmatisation**: reducing words/tokens into their base form. \n",
    "\n",
    "> #### Lemmatisation vs stemming\n",
    ">   There are minor diffrences between Lemmatisation and **stemming** that was\n",
    ">   introduced in the lecture. Stemming usually refers to a heuristic process\n",
    ">   that chops off the ends of words to get the base forms of the words.\n",
    ">   Lemmatisation usually refers the process of doing similar things, but using\n",
    ">   additional information of how the word is used in the context (semantic\n",
    ">   information). Lemmatisation returns the dictionary form of a word, also\n",
    ">   known as lemma. Given the word \"ponies\" for example, stemming will return\n",
    ">   \"poni\" (incorrect), while lemmatisation will return \"pony\" (correct). The\n",
    ">   drawback of lemmatisation is it requires additional analysis to obtain the\n",
    ">   required semantic information.\n",
    ">   [Source](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise WordNet lemmatizer and punctuation filter\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "punct = set(string.punctuation)\n",
    "\n",
    "# load the provided stopwords\n",
    "df_stop = pd.read_json('datasets/Federaliststop.json')\n",
    "\n",
    "# join provided stopwords with the default NLTK English stopwords\n",
    "stopwords = set(df_stop['Term']).union(set(sw.words('english')))\n",
    "\n",
    "# tf idf vectoriser\n",
    "tfidf_vec = TfidfVectorizer(tokenizer=cab_tokenizer, ngram_range=(1,2))\n",
    "X = tfidf_vec.fit_transform(df['Text'])\n",
    "\n",
    "# see the number of unique tokens produced by the vectorizer. Lots of them...\n",
    "print(len(tfidf_vec.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Document Analysis</h3>\n",
    "<h4>Kmeans</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Elbow and sollouet rule to get optimal K\n",
    "# {Not Included}\n",
    "# K means clustering using the term vector\n",
    "kmeans = KMeans(n_clusters=7, random_state=rs).fit(X)\n",
    "\n",
    "# visualize it\n",
    "visualise_text_cluster(kmeans.n_clusters, kmeans.cluster_centers_, tfidf_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Feature Selection and Transformation</h3>\n",
    "<h4>Zipf's Law and Document Frequency Filtering</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = calculate_tf_idf_terms(df['Text'])\n",
    "\n",
    "visualise_zipf(terms)\n",
    "\n",
    "# another tf idf vectoriser\n",
    "# limit the terms produced to terms that occured in min of 2 documents and max 80% of all documents\n",
    "filter_vec = TfidfVectorizer(tokenizer=cab_tokenizer, ngram_range=(1,2), min_df=2, max_df=0.8)\n",
    "X_filter = filter_vec.fit_transform(df['Text'])\n",
    "\n",
    "# see the number of unique tokens produced by the vectorizer. Reduced!\n",
    "print(len(filter_vec.get_feature_names()))\n",
    "\n",
    "## TESTING THE FILTER ##\n",
    "# K means clustering using the new term vector, time it for comparison to SVD\n",
    "kmeans_fil = KMeans(n_clusters=7, random_state=rs).fit(X_filter)\n",
    "# visualisation\n",
    "visualise_text_cluster(kmeans_fil.n_clusters, kmeans_fil.cluster_centers_, filter_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Singlular Value Decompisition</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=100, random_state=42)\n",
    "X_trans = svd.fit_transform(X_filter)\n",
    "\n",
    "# sort the components by largest weighted word\n",
    "sorted_comp = svd.components_.argsort()[:, ::-1]\n",
    "terms = filter_vec.get_feature_names()\n",
    "\n",
    "# visualise word - concept/component relationships\n",
    "for comp_num in range(10):\n",
    "    print(\"Top terms in component #{}\".format(comp_num), end=\" \")\n",
    "    for i in sorted_comp[comp_num, :5]:\n",
    "        print(terms[i], end=\", \")\n",
    "    print()\n",
    "    \n",
    "# K-means clustering using LSA-transformed X\n",
    "svd_kmeans = KMeans(n_clusters=7, random_state=rs).fit(X_trans)\n",
    "\n",
    "# transform cluster centers back to original feature space for visualisation\n",
    "original_space_centroids = svd.inverse_transform(svd_kmeans.cluster_centers_)\n",
    "\n",
    "# visualisation\n",
    "visualise_text_cluster(svd_kmeans.n_clusters, original_space_centroids, filter_vec.get_feature_names())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
